{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\n\n# Load the data from CSV\ndata_ = pd.read_csv('Train_test_combined.csv')\n\n# Extract feature data (excluding the first column if it's non-numeric or an index)\ndata = data_.iloc[:, 1:]\n\n# Define labels\nlabel1 = np.ones((1288, 1))  # Value can be changed\nlabel2 = np.zeros((1133, 1))\nlabel3 = np.ones((258, 1))   # Value can be changed\nlabel4 = np.zeros((227, 1))\n\n# Split the data into training and testing sets\nX_train = data.loc[0:2420]\ny_train = np.concatenate((label1, label2))\n\nX_test = data.loc[2421:]\ny_test = np.concatenate((label3, label4))\n\n# Scale the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert the scaled data to DataFrames\ndf_train = pd.DataFrame(X_train_scaled)\ndf_test = pd.DataFrame(X_test_scaled)\n\n# Remove features with zero standard deviation in the training data\nzero_std_features = df_train.columns[df_train.std() == 0].tolist()\ndf_train = df_train.loc[:, df_train.std() != 0]\n\n# Apply the same feature exclusion to the test data\ndf_test = df_test.loc[:, df_train.columns]\n\n# Drop highly correlated features (correlation > 0.8) in the training data\nthreshold = 0.8\ncor = df_train.corr().abs()\nhigh_corr_var = np.where(cor > threshold)\nhigh_corr_var = [(cor.columns[x], cor.columns[y]) for x, y in zip(*high_corr_var) if x != y and x < y]\n\nto_drop = set()\nfor pair in high_corr_var:\n    to_drop.add(pair[1])\n\ndf_train = df_train.drop(columns=to_drop)\ndf_test = df_test.drop(columns=to_drop)\n\n# Convert the feature-selected training and testing data back to numpy arrays\nX_train_selected = df_train.values\nX_test_selected = df_test.values\n\n# Fit ElasticNetCV to the training data for feature selection\nmodel_cv = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], alphas=[0.01, 0.1, 1.0, 10.0], cv=10)\nmodel_cv.fit(X_train_selected, y_train)\n\n# Train the ElasticNet model with the best alpha and l1_ratio on the training data\nmodel = ElasticNet(alpha=model_cv.alpha_, l1_ratio=model_cv.l1_ratio_)\nmodel.fit(X_train_selected, y_train)\n\n# Get the importance scores (coefficients) from the trained model\nimportance = model.coef_\n\n# Identify the selected feature indices (non-zero coefficients)\nselected_indices = np.where(importance != 0)[0]\n\n# Create a DataFrame with feature importance, original indices, and absolute importance scores\nimportance_df = pd.DataFrame({\n    'feature_index': selected_indices,\n    'importance_score': importance[selected_indices],\n    'absolute_importance': np.abs(importance[selected_indices])\n})\n\n# Rank the importance scores based on the absolute values\nimportance_df['rank'] = importance_df['absolute_importance'].rank(ascending=False)\n\n# Sort the DataFrame by the rank\nimportance_df = importance_df.sort_values(by='rank')\n\n# Save the importance scores, indices, and ranks to a CSV file\nimportance_df.to_csv('Clathrin10_EN_importance_ranked.csv', index=True)\n\n# Select the important features based on the selected indices for both training and testing sets\nX_train_important = X_train_selected[:, selected_indices]\nX_test_important = X_test_selected[:, selected_indices]\n\n# Combine the training and testing data with the correct indices from the original CSV\ncombined_data = np.concatenate((X_train_important, X_test_important), axis=0)\n\n# Create a DataFrame with the combined data\ndf_combined = pd.DataFrame(combined_data)\n\n# Use only the indices corresponding to the combined training and test samples\ncombined_indices = pd.concat([data_.iloc[0:2421, 0], data_.iloc[2421:, 0]])  # First column of original data assumed to be index or identifier\n\n# Assign the correct index to the combined DataFrame\ndf_combined.index = combined_indices\n\n# Save the combined DataFrame with selected features and the correct index to a CSV file\ndf_combined.to_csv('Clathrin10_EN_features.csv', index=True, header=True)\n\nprint(\"Shape of the combined feature set:\", np.shape(df_combined))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}