{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ESM1280\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install fair-esm==2.0.0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import esm\nimport torch\nimport pandas as pd\nimport gc\nimport os\n\ndef esm_embeddings(peptide_sequence_list, model_name, batch_size=1, chunk_size=50):\n    model_dict = {\n        'esm2_t36_3B_UR50D': (esm.pretrained.esm2_t36_3B_UR50D, 36),\n        'esm2_t33_650M_UR50D': (esm.pretrained.esm2_t33_650M_UR50D, 33),\n    }\n\n    if model_name not in model_dict:\n        raise ValueError(f\"Invalid model name '{model_name}'. Please choose from {list(model_dict.keys())}.\")\n\n    model_func, num_layers = model_dict[model_name]\n    model, alphabet = model_func()\n\n    batch_converter = alphabet.get_batch_converter()\n    model.eval()\n\n    device = torch.device('cpu')  # Switch to CPU\n    model = model.to(device)\n\n    embeddings_results = []\n\n    for start in range(0, len(peptide_sequence_list), chunk_size):\n        end = start + chunk_size\n        chunk_sequences = peptide_sequence_list[start:end]\n\n        for i in range(0, len(chunk_sequences), batch_size):\n            batch_sequences = chunk_sequences[i:i + batch_size]\n            batch_labels, batch_strs, batch_tokens = batch_converter(batch_sequences)\n            batch_tokens = batch_tokens.to(device)\n\n            with torch.no_grad():\n                results = model(batch_tokens, repr_layers=[num_layers])\n\n            token_representations = results[\"representations\"][num_layers]\n            batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n\n            for j, tokens_len in enumerate(batch_lens):\n                sequence_representation = token_representations[j, 1 : tokens_len - 1].mean(0)\n                embeddings_results.append(sequence_representation.cpu().numpy())\n\n            del batch_tokens, results, token_representations, batch_sequences\n            torch.cuda.empty_cache()\n            gc.collect()\n\n        chunk_df = pd.DataFrame(embeddings_results)\n        chunk_df.columns = [f\"ESM1280_{i}\" for i in range(chunk_df.shape[1])]\n        chunk_df.to_csv(f'ESM1280_chunk_{start//chunk_size}.csv', index=False)\n        embeddings_results = []\n        gc.collect()  # Explicitly run garbage collection\n\n    final_embeddings = pd.concat([pd.read_csv(f'ESM1280_chunk_{i}.csv') for i in range(len(peptide_sequence_list) // chunk_size + 1)], ignore_index=True)\n    return final_embeddings\n\n# Load the dataset\ndataset = pd.read_csv('clathrin0-7/Clathrin07.csv', na_filter=False)\nsequence_list = dataset['seq']\npeptide_sequence_list = []\n\n# Prepare sequence_list for ESM processing\nfor seq in sequence_list:\n    format_seq = [seq, seq]\n    peptide_sequence_list.append(tuple(format_seq))\n\n# Generate embeddings and save to CSV\nembeddings_results = esm_embeddings(peptide_sequence_list, 'esm2_t33_650M_UR50D', batch_size=1, chunk_size=50)\nembeddings_results.to_csv('ESM1280.csv', index=True)\n\n# Optional: remove intermediate chunk files to free disk space\nfor i in range(len(peptide_sequence_list) // 50 + 1):\n    os.remove(f'ESM1280_chunk_{i}.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ProtT5\n### - ProtT5\n### - ProtT5_xl_uniref50 \n### - ProtT5_xl_bfd","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5EncoderModel, BertTokenizer, BertModel\nimport re\nimport torch\nimport pandas as pd\nimport gc\nfrom torch.cuda.amp import autocast\n\ndef generate_protein_embeddings(sequence_list, model_name, batch_size=1, max_length=512):\n    if model_name not in [\n        'Rostlab/ProstT5', \n        'Rostlab/prot_t5_xl_uniref50', \n        'Rostlab/prot_t5_xl_bfd']:\n        raise ValueError(\"Invalid model name. Please choose a valid model.\")\n    \n    sequence_list = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequence_list]\n    sequence_lengths = [len(sequence) for sequence in sequence_list]\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    \n    if \"bert\" in model_name:\n        tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n        model = BertModel.from_pretrained(model_name).to(device)\n    else:\n        tokenizer = T5Tokenizer.from_pretrained(model_name, do_lower_case=False)\n        model = T5EncoderModel.from_pretrained(model_name).to(device)\n    \n    if device.type == 'cpu':\n        model.float()\n    else:\n        model.half()\n    \n    embeddings_list = []\n    \n    for i in range(0, len(sequence_list), batch_size):\n        batch_sequences = sequence_list[i:i+batch_size]\n        batch_lengths = sequence_lengths[i:i+batch_size]\n        \n        batch_sequences = [\" \".join(list(sequence)) for sequence in batch_sequences]\n        ids = tokenizer(batch_sequences, add_special_tokens=True, padding=\"longest\", truncation=True, max_length=max_length)\n        input_ids = torch.tensor(ids['input_ids']).to(device)\n        attention_mask = torch.tensor(ids['attention_mask']).to(device)\n        \n        with torch.no_grad(), autocast():\n            embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)\n        \n        embeddings = [embedding_repr.last_hidden_state[j, :length] for j, length in enumerate(batch_lengths)]\n        per_protein_embeddings = [emb.mean(dim=0) for emb in embeddings]\n        embeddings_list.extend([emb.cpu().numpy() for emb in per_protein_embeddings])\n        \n        del input_ids, attention_mask, embedding_repr, embeddings, per_protein_embeddings\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    embeddings_df = pd.DataFrame(embeddings_list)\n    return embeddings_df\n\n# Load the dataset\ndataset = pd.read_csv('clathrin0-7/Clathrin07.csv', na_filter=False) \n#column_names = ['Column1', 'seq'] \n#dataset = pd.read_csv('/kaggle/input/clathrin0-7/Clathrin07.csv', header=None, names=column_names, na_filter=False)\n\n\nsequence_list = dataset['seq']\n\n# Divide the sequences into 20 equal parts\nnum_parts = 20\nchunk_size = len(sequence_list) // num_parts\nchunks = [sequence_list[i:i+chunk_size] for i in range(0, len(sequence_list), chunk_size)]\n\n# List of models to generate embeddings\nmodel_list = [\n    'Rostlab/ProstT5',\n    'Rostlab/prot_t5_xl_bfd',\n    'Rostlab/prot_t5_xl_uniref50'\n]\n\n# Generate embeddings for each chunk and each model\nfor model_name in model_list:\n    all_embeddings = []\n    for chunk_index, chunk in enumerate(chunks):\n        print(f\"Processing chunk {chunk_index+1}/{num_parts} for model {model_name}...\")\n        embeddings_df = generate_protein_embeddings(chunk, model_name, batch_size=2, max_length=256)  # Adjusted batch size and max length\n        all_embeddings.append(embeddings_df)\n    \n    # Combine all embeddings and save to CSV\n    final_embeddings_df = pd.concat(all_embeddings, ignore_index=True)\n    final_embeddings_df.columns = [f\"{model_name.split('/')[-1]}_{i}\" for i in range(final_embeddings_df.shape[1])]\n    final_embeddings_df.to_csv(f'{model_name.split(\"/\")[-1]}.csv', index=True, header=True)\n\nprint(\"Embeddings generated and saved for all models.\")\n","metadata":{},"execution_count":null,"outputs":[]}]}